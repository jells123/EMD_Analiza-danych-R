---
title: "Śledzie"
author: "Urszula Walińska 127216, Julia Będziechowska 127306"
date: '`r format(Sys.Date(), "%d.%m.%Y")`'
output: html_document
---

# Spis treści
1. [Streszczenie analizy](#streszczenie)
2. [Wczytanie i przygotowanie danych](#wczytanie)
3. [Podstawowe informacje o zbiorze danych](#podst_info)
4. [Analiza wartości atrybutów](#wartosci_atryb)
5. [Korelacje między zmiennymi](#korelacje)
6. [Zmiana rozmiaru śledzi w czasie](#rozmiar)
7. [Regresor przewidujący rozmiar śledzia](#regresor)
    1. [Przygotowanie zbioru trenującego i testowego](#podzial)
    2. [Regresja liniowa](#linear-regression)
    3. [Regresja nieliniowa (wielomianowa)](#nonlinear-regression)
    4. [Regresja Ridge, Lasso i Elastic-Net](#ela-regression)
8. [Analiza ważności atrybutów najlepszego modelu regresji]

## Streszczenie analizy <a name="streszczenie"></a>
Zabawa była przednia

## Wczytanie i przygotowanie danych <a name="wczytanie"></a>

Lista wykorzystanych bibliotek:

- dplyr  
- ggplot2  
- corrplot  
- plotly

```{r, echo=FALSE, results='hide', message=FALSE}
library(corrplot)
library(dplyr)
library(ggplot2)
library(plotly)
library(glmnet)
library(gbm)

knitr::opts_chunk$set(cache=TRUE)

printf <- function(...) invisible(print(sprintf(...)))
```
```{r, echo=FALSE}
(.packages())
```

Kod zapewniający powtarzalność wyników przy każdym uruchomieniu raportu na tych samych danych:

```{r}
set.seed(13)
```

Wczytanie danych i podgląd postawowych informacji na temat zbioru danych: 

```{r}
raw_data <- read.csv("sledzie.csv")
raw_data <- raw_data %>% 
              rename(t = X)

all_rows_number <- nrow(raw_data)
str(raw_data)
```

Z rezultatu wywołania funkcji `str` wynika, że niektóre kolumny zawierają znaki zapytania, co oznacza brakujące dane. 

```{r}
raw_data[raw_data == "?"] <- NA
missing_rows <- raw_data[!complete.cases(raw_data),]
data <- raw_data[complete.cases(raw_data),]
```

Przetwarzanie brakujących wartości:

Każdą brakującą wartość, oryginalnie reprezentowaną jako znak zapytania, zamieniamy na charakterystyczne dla języka R wyrażenie `NA`. Dzięki temu możliwe jest wykorzystanie funkcji `complete_cases`. 

Podsumowanie liczby wierszy w zbiorze danych przed i po pominięciu wierszy z brakującymi danymi:
```{r, echo=FALSE}
rows_number <- nrow(data)
printf("Przed: %d, po usunięciu wierszy zawierających '?': %d", all_rows_number, rows_number)
```

## Podstawowe informacje o zbiorze danych <a name="podst_info"></a>
Zbiór danych powstał na potrzeby analizy rozmiaru śledzi oceanicznich wyławianych w Europie, w zależności od warunków w jakich żyją. Zauważono bowiem, że na przestrzeni ostatnich lat ich rozmiar stopniowo maleje i zaczęto się tym faktem niepokoić. Do analizy zebrano pomiary śledzi i warunków w jakich żyją z ostatnich 60 lat. Dane były pobierane z połowów komercyjnych jednostek, a w ramach połowu jednej jednostki wybierano losowo od 50 do 100 sztuk trzyletnich śledzi.

Wiersze w zbiorze są uporządkowane chronologicznie, dlatego przyjęto że liczba porządkująca wiersze będzie utożsamiana z czasem.

```{r}
str(raw_data)
```
Jak widać w rezultacie wywołania powyższej metody, oryginalne, surowe dane zawierają 52582 przykłady opisane przez 16 atrybutów (15 właściwych i 1 porządkujący). Ich dokładniejsza analiza zostanie przedstawiona w dalszej części raportu. W niektorych przypadkach możemy zaobserwować brak pomiaru wartości danego atrybutu oznaczony przez znak zapytania.

Są to zmiene dotyczące dostępności planktonu i temperatury przy powierzchni wody. Dokładne informacje dotyczące brakujących danych z uwzględnieniem kolumn przedstawia poniższa komórka:

```{r, echo=FALSE}
for(cname in colnames(missing_rows)) { printf("%s: %d brakujących wartości", cname, sum(is.na(missing_rows[cname]))) }
```

Łącznie daje to nam około 10000 niepełnych przykładów, które zdecydowałyśmy się odfiltrować, aby ułatwić dalszą analizę. Z powodu braku niektórych wartości i oznaczenia ich przez "?" typ niektórych zmiennych został domyślnie ustawiony na Factor (typ wyliczeniowy, może być interpretowany jako kategoria). Widzimy jednak, że pozostałe wartości są numeryczne (całkowite lub niecałkowite), tak więc w dalszej analizie, po odfiltrowaniu niepełnych przykładów, dla ułatwienia zmienimy typ tych zmiennych na numeryczny. R domyślnie ustala typ zmiennych całkowitych na *integer*, a niecałkowitych na *numeric*.

```{r}
dim(data)
```

Po odfiltrowaniu brakujących wartości, zbiór danych składa się z 42488 kompletnych przykładów, każdy opisany przez 16 atrybutów. Poniżej wyświetlonych zostało kilka przykładów ze zbioru.

```{r}
head(data)
```

## Analiza wartości atrybutów <a name="atrybuty"></a>

Poniżej przedstawiono analizę wartości poszczególnych atrybutów, które opisują dany rekord. Łącznie danych jest 15 atrybutów. Wśród nich występują zmienne takie jak dostępności różnych gatunków planktonu, informacje dotyczące połowów, właściwości wody etc. 

Lista atrybutów jest następująca:

/ # TODO ADD LINKS

- **length**: długość złowionego śledzia [cm];
- **cfin1**: dostępność planktonu [zagęszczenie Calanus finmarchicus gat. 1];
- **cfin2**: dostępność planktonu [zagęszczenie Calanus finmarchicus gat. 2];
- **chel1**: dostępność planktonu [zagęszczenie Calanus helgolandicus gat. 1];
- **chel2**: dostępność planktonu [zagęszczenie Calanus helgolandicus gat. 2];
- **lcop1**: dostępność planktonu [zagęszczenie widłonogów gat. 1];
- **lcop2**: dostępność planktonu [zagęszczenie widłonogów gat. 2];
- **fbar**: natężenie połowów w regionie [ułamek pozostawionego narybku];
- **recr**: roczny narybek [liczba śledzi];
- **cumf**: łączne roczne natężenie połowów w regionie [ułamek pozostawionego narybku];
- **totaln**: łączna liczba ryb złowionych w ramach połowu [liczba śledzi];
- **sst**: temperatura przy powierzchni wody [°C];
- **sal**: poziom zasolenia wody [Knudsen ppt];
- **xmonth**: miesiąc połowu [numer miesiąca];
- **nao**: oscylacja północnoatlantycka [mb].

```{r echo=FALSE}
numeric_data <- data.frame(sapply(data, as.numeric))
```

### Długość złowionego śledzia (length) <a name="attr_length"></a>

```{r echo=FALSE}

# TODO: ADD BOXPLOTS!

analyze_distribution <- function(values, label, hist_color="#28A5A9") {
  df <- data.frame(v=values)
  p <- ggplot(df, aes(x=v)) + geom_bar(fill=hist_color, color='black')
  title <- sprintf("Histogram liczebności zmiennej '%s'", label)
  print(p + ggtitle(title))
  print(summary(values))
  printf("Odchylenie standardowe: %f", sd(values))
}

p <- ggplot(numeric_data, aes(x=length)) + geom_histogram(binwidth=0.5, fill="#28A5A9", alpha=0.7, color='black') + scale_x_continuous(breaks=seq(19,32.5, by=1))
print(p + ggtitle("Histogram liczebności zmiennej 'length'"))
summary(numeric_data$length)
printf("Odchylenie standardowe: %f", sd(numeric_data$length))

# TODO smoothen, apply median filter?
# plot(numeric_data$length, type='l')
```

Histogram dystrybucji zmiennej *length*, jak i postawowe statystyki które ją opisują świadczą o tym że ta zmienna ma rozkład normalny. Średnia oraz media są sobie prawie równe. Sama rozpiętość zmiennej jest stosunkowo mała, najmniejsza zaobserwowana długość to 19 centymetrów, największa - 32.5 centymetra. Należało się tego spodziewać, ponieważ zmienna ta opisuje długość śledzia, a ponadto wiadomo o nich że wszystkie miały około 3 lat. Tę zmienną charakteryzuje także niskie odchylenie standardowe.

### Dostępność planktonu -  zagęszczenie Calanus finmarchicus <a name="attr_cfin"></a>
```{r echo=FALSE}
analyze_distribution(numeric_data$cfin1, 'cfin1', "#1FE4EB")

cfin_eq_min <- numeric_data$cfin1[numeric_data$cfin1 == min(numeric_data$cfin1)]
min_ratio = nrow(data.frame(x=cfin_eq_min)) / rows_number
printf("Procent dla którego zmienna osiągała wartość równą minimum: %f", min_ratio)
```
Na histogramie dystrybucji zmiennej *cfin1* widoczna jest zdecydująca przewaga bardzo niskich wartości, bliskich minimum. W prawie 30% przypadkach wartość tej zmiennej wynosiła 2.0. Maksymalne zaobserwowane zagęszczenie wyniosło 40.0. Odchylenie standardowe jest wysokie i rozkład zmiennej nie przypomina żadnego charakterystycznego rozkładu.

```{r echo=FALSE}
analyze_distribution(numeric_data$cfin2, 'cfin2', "#2D8F92")
```
W przypadku zmiennej *cfin2* także wiele razy zaobserwowano wartości bliskie minimum, jednak nie tak dużo jak w przypadku *cfin1*. Średnia jak i mediana są w tym przypadku wyższe. Rozpiętość zmiennej jest zbliżona: wartość minimalna wynosi 2.0 a maksymalna 49.0. Zauważalne są trzy słupki histogramu znacznie wyższe od pozostałych. Zmienna ma niestandardowy rozkład i wysokie odchylenie standardowe. 

### Dostępność planktonu -  zagęszczenie Calanus helgolandicus <a name="attr_chel"></a>
```{r echo=FALSE}
analyze_distribution(numeric_data$chel1, 'chel1', "#21DB59")
```
Histogram dystrybucji zmiennej *chel1* ukazuje, że ta zmienna także ma niestandardowy rozkład. Rozkład ten kumuluje się wokół wartości 13.0, co widać na histogramie jako wysoki słupek. Podstawowe statystyki (w szczególności minimum i maksimum) są zbliżone do tych opisujących zmienną *cfin2*. 

```{r echo=FALSE}
analyze_distribution(numeric_data$chel2, 'chel2', "#1D9E44")
```
Podobnie dla zmiennej *chel2*, rozkład jest nieregularny i odchylenie standardowe jest wysokie. Ta zmienna dostępności planktonu ma stosunkowo wysoką średnią. Widoczne są dwa wysokie słupki, tym razem część wartości kumuluje się około liczby 24 (przedział 23-24) i 50 (przedział 49-50). 

### Dostępność planktonu -  zagęszczenie widłonogów <a name="attr_lcop"></a>
```{r echo=FALSE}
analyze_distribution(numeric_data$lcop1, 'lcop1', "#FF6666")
```
Charakterystyka rozkładu zmiennej *lcop1* jest podobna jak w przypadku innych zmiennych dotyczących dostępności plakntonu. Zmienna przyjmuje różnorodne wartości; najwięcej wartości przyjmuje w przedziale 22-23, co można zaobserwować w postaci wysokiego słupka na histogramie.

```{r echo=FALSE}
analyze_distribution(numeric_data$lcop2, 'lcop2', "#C43F3F")
```
Także w przypadku zmiennej *lcop2*, jej rozkład jest nieregularny i jej wartości kumulują się w wybranych przedziałach (20-21 oraz 51-52). Odchylenie standardowe jest ponownie wysokie. Zakres wartości tej zmiennej jest podobny jak w przypadku poprzednich zmiennych opisujących dostępność planktonu. 

```{r , echo=FALSE} 
data_stacked <- stack(numeric_data)
planktons <- data_stacked[(data_stacked['ind'] == 'cfin1' | data_stacked['ind'] == 'cfin2' | data_stacked['ind'] == 'chel1' | data_stacked['ind'] == 'chel2' | data_stacked['ind'] == 'lcop1' | data_stacked['ind'] == 'lcop2'),]
p <- ggplot(planktons, aes(x = ind, y = values, fill=ind)) + geom_boxplot(notch=TRUE, outlier.colour="red", outlier.shape=8, outlier.size=4)

print(p + ggtitle("Wykresy pudełkowe dla zmiennych opisujących dostępność planktonu") )
```

Z uwagi na to, że sześć zmiennych opisujących dostępność planktonu ma podobne charakterystyki (co wynika z tego, co reprezentują), na powyższym wykresie zbiorczo przedstawiono odpowiadające im wykresy pudełkowe. Na wykresie nie są widoczne żadne wartości skrajne, tzw. outliery. Zmienne charakteryzują się podobną rozpiętością, a ich średnie nieznacznie się różnią. Można podejrzewać, że plankton związany z zmienną *cfin1* jest zazwyczaj najmniej liczny, a dominującymi gatunkami są *chel2* i *lcop2*. 

### Natężenie połowów w regionie (fbar) [ułamek pozostawionego narybku] <a name="attr_fbar"></a>

```{r echo=FALSE}
p <- ggplot(numeric_data, aes(x=fbar)) + geom_histogram(binwidth=0.03, fill="#7D4190", alpha=0.7, color='black')
print(p + ggtitle("Histogram liczebności zmiennej 'fbar'"))
summary(numeric_data$fbar)
printf("Odchylenie standardowe: %f", sd(numeric_data$fbar))
```
Średnie natężenie połowów w regionie wynosi 0.3306, co oznacza że średnio około 33% narybku nie zostaje złowione. Z histogramu wynika, że zdarzyły się także (rzadkie) przypadki w których ten współczynnik był bliski maksimum (około 85%). Rozkład zmiennej jest nieregularny, jednak można zaobserwować tendencję to zmniejszania się liczności wystąpień zmiennej im większy ułamek pozostawionego narybku. Większość przypadków nie przekracza 50%.

### Roczny narybek (recr) [liczba śledzi] <a name="attr_recer"></a>

```{r echo=FALSE}
p <- ggplot(numeric_data, aes(x=recr)) + geom_histogram(binwidth=50000, color='black')
print(p + ggtitle("Histogram liczebności zmiennej 'recr'"))
summary(numeric_data$recr)
printf("Odchylenie standardowe: %f", sd(numeric_data$recr))
```
Zmienna *recr* przyjmuje bardzo wysokie wartości, są to liczby rzędu kilkuset tysięcy. Zaobserwowane maksimum wynosi ponad półtora miliona. Również w tym przypadku odchylenie standardowe jest wysokie. Przeważają wartości rzędu 400 000 śledzi. Zmienna przyjmuje bardzo rożne wartości co jest stosunkowo zaskakujące, biorąc pod uwagę co reprezentuje ta zmienna. Z histogramu wynika, że zdarzały się lata które skutowały w bardzo dużej liczbie narybku, nawet przekraczające średnią dwukrotnie. Można wyróżnić trzy grupy słupków na histogramie, oddzielone przerwami, tj. brakiem wystąpień. Pierwsza z nich jest najbardziej liczna.

### Łączne roczne natężenie połowów w regionie (cumf) [ułamek pozostawionego narybku] <a name="attr_cumf"></a>

```{r echo=FALSE}
p <- ggplot(numeric_data, aes(x=cumf)) + geom_histogram(binwidth=0.02, fill="#6A5CA2", alpha=0.8, color='black')
print(p + ggtitle("Histogram liczebności zmiennej 'cumf'"))
summary(numeric_data$cumf)
printf("Odchylenie standardowe: %f", sd(numeric_data$cumf))
```
Zmienna *cumf* jest powiązana ze zmienną *fbar* - natężenie połowów w regionie, ponieważ dotyczy tej samej obserwacji (ułamku pozostawionego narybku), tylko kumuluje wszystkie wartości zaobserwowane w danym roku. Średnie łączne natężenie połowów wynosi około 23% pozostawionego narybku. Jest to mniej niż wynika ze zmiennej *fbar*. Można zaobserwować że mimo wystąpienia dla zmiennej *fbar* wartości skrajnie wysokich, w tym przypadku zakres wartości zmiennej jest już mniejszy. Maksymalne łączne rozczne natężenie połowów w regionie wynosiło niecałe 40%.

### Łączna liczba ryb złowionych w ramach połowu (totaln) [liczba śledzi] <a name="attr_totaln"></a>

```{r echo=FALSE}
p <- ggplot(numeric_data, aes(x=totaln)) + geom_histogram(binwidth=50000, fill="#ECBA57", color='black')
print(p + ggtitle("Histogram liczebności zmiennej 'totaln'"))
summary(numeric_data$totaln)
printf("Odchylenie standardowe: %f", sd(numeric_data$totaln))
```
Podobnie jak zmienna *recr*, także wyrażana w liczbie śledzi, zmienna *totaln* osiąga wartości rzędu kilkuset tysięcy, z maksymalną wartością przekraczającą milion. Najwięcej razy ta liczba była rzędu 750 000, co odzwierciedla wysoki słupek na wyrkesie histogramu. Zdarzyły się przypadki połowów w których liczba śledzi była dwa razy większa niż ich średnia liczba.

### Temperatura przy powierzchni wody (sst) [°C] <a name="attr_sst"></a>

```{r echo=FALSE}
p <- ggplot(numeric_data, aes(x=sst)) + geom_histogram(binwidth=5, fill="#BFB0BB", color='black')
print(p + ggtitle("Histogram liczebności zmiennej 'sst'"))
summary(numeric_data$sst)
printf("Odchylenie standardowe: %f", sd(numeric_data$sst))
```
Rozkład wartości tej zmiennej jest stosunkowo zaskakujący, ponieważ wynika z niego że często obserwowane były zarówno bardzo niskie jak i bardzo wysokie temperatury wody. Szczególnie maksimum jest zastanawiające - największa wartość tej zmiennej to 52 stopnie celsjusza. Należy brać pod uwagę fakt że pomiar następował przy powierzchni wody, w związku z czym wpływ na taki pomiar z pewnością miało na przykład nasłonecznienie. Średnia temperatura przy powierzchni wody wynosi 25 stopni. Stosunkowo rzadko odnotowano temperatury bliskie zera. Dwa przedziały o największej liczności, jak wynika z histogramu, to 30-35 oraz 10-15. 

### Poziom zasolenia wody (sal) [Knudsen ppt] <a name="attr_sal"></a>

```{r echo=FALSE}
p <- ggplot(numeric_data, aes(x=sal)) + geom_histogram(binwidth=0.025, fill="#A9D7D4", color="black")
print(p + ggtitle("Histogram liczebności zmiennej 'sal'"))
summary(numeric_data$sal)
```
Zmienna *sal* przyjmuje wartości z bardzo małego przedziału, w porównaniu do innych atrybutów tego zbioru danych. Wartości minimalne i maksymalne niewiele różnią się od wartości średniej. Z hsitogramu można wywnioskować, że większość obserwacji przyjęło wartości bliskie średniej (lub mediany, są one sobie równe). Jednak ponadto, część wartości kupuluje się albo przy minimum, albo przy maksimum. Najmniej wystąpień jest wartości pośrednich.

### Miesiąc połowu (xmonth) [numer miesiąca] <a name="attr_xmonth"></a>

```{r echo=FALSE}
months_breaks <- factor(1:12)
months_labels <- c("sty ", "lut", "mar", "kwi", "maj", "cze", "lip", "sie", "wrz", "paz", "lis", "gru")

p <- ggplot(numeric_data, aes(x=factor(xmonth))) + geom_histogram(fill="#98F0CD", color="black", stat="count") + scale_x_discrete(breaks=months_breaks, labels=months_labels) 
print(p + ggtitle("Histogram liczebności zmiennej 'xmonth'"))
summary(numeric_data$xmonth)
```
Najwięcej obserwacji odnotowano dla miesiąca sierpień, w następnej kolejności: październik i lipiec. Najmniej obserwacji jest danych dla stycznia. Warto zwrócić uwagę, że rozkład miesięcy jest nierównomierny. Biorąc dalej pod uwagę, że każdy miesiąc charakteryzują pewne warunki pogodowe, należy uwzględniać taki fakt na przykład przy interpretowaniu średniej temperatury przy powierzchni wody. Wartości z miesiąca sierpień mogłyby zawyżać średnią w związku z tym, że danych jest najwięcej rekordów reprezentujących ten miesiąc. W ogólności z histogramu wynika, że dominują miesiące letnie oraz poprzedzające i następujące ten okres. 

### Oscylacja północnoatlantycka (nao) [mb] <a name="attr_nao"></a>

```{r echo=FALSE}
p <- ggplot(numeric_data, aes(x=nao)) + geom_histogram(fill="#003333", alpha=0.75, color='black', binwidth=1)
print(p + ggtitle("Histogram liczebności zmiennej 'nao'"))
summary(numeric_data$nao)
printf("Odchylenie standardowe: %f", sd(numeric_data$nao))
```
Zmienna *nao* jako jedyna przyjmuje wartości ujemne i dodatnie. Jest to zgodne z zakresem wartości jakie przyjmuje tzw. indeks NAO, który może reprezentować wystąpienie dodatniej lub ujemnej *fazy NAO*. Oscylacja północnoatlantycka jest zjawiskiem meteorologicznym wpływającym na klimat, które manifestuje się poprzez fluktuacje ciśnienia atmosferycznego, temperatury powietrza, prędkości wiatru, ilości opadów i innych parametrów [wikipedia(https://pl.wikipedia.org/wiki/Oscylacja_p%C3%B3%C5%82nocnoatlantycka)]. 
Na histogramie widać, że rozkład tej zmiennej byłby zbliżony do normalnego, gdyby nie niska liczność wartości w przedziale -2 do -1. Zarówno wartości skrajnie niskie jak i wysokie występują stosunkowo rzadko. Średnia wartość jest bliska zeru. Ta zmienna także ma dość wysokie odchylenie standardowe.

## Korelacje między zmiennymi <a name="korelacje"></a>

```{r}
corrplot(cor(numeric_data))
```

Obliczenie wartości współczynnika korelacji pomiędzy zmiennymi, a zwłaszcza korelacji zmiennej *length* z innymi może zdecydowanie pomóc w odnalezieniu ukrytych zależności prowadzącymi na przykład do odkrycia jakie czynniki mają wpływ na zmniejszenie długości śledzi w ostatnich latach. Współczynnik korelacji przyjmuje wartości od −1 (zupełna korelacja ujemna), przez 0 (brak korelacji) do +1 (zupełna korelacja dodatnia).

Na powyższej macierzy korelacji możemy zaobserwować następujące zależności: 

- silna korelacja dodatnia (około 0.9) pomiędzy zmiennymi *fbar* oraz *cumf*. Obywie te zmienne oznaczają ułamek pozostawionego narybku, *cumf* jest natomiast jego skumulowaną roczną wersją, a więc logicznie, wraz ze wzrostem *fbar* rośnie *cumf* tak więc jest to korelacja dodatnia
- silna korelacja ujemna (około -0.8) pomiędzy zmiennymi *totaln* i *cumf*, oraz nieco słabsza (około -0.6) *totaln* i *fbar*. *totaln* oznacza łączną liczbę śledzi złowionych w ramach połowu. Jest to oczywiście wartość odwrotnie proporcjonalna do ułamku pozostawionego narybku w danym czasie.  
- korelacja dodatnia *chel1* i *lcop1* oraz *chel2* i *lcop2*, które oznaczają zagęszczenie różnych gatunków planktonu. Być może sugeruje to fakt iż wymienione powyżej gatunki kooperują ze sobą i dlatego żyją zagęszczenie działa w sposób pozytywny na zagęszczenie drugich.  
- możemy dostrzec również słabą ujemną zależność długości śledzia od czasu, co potwierdza fakt, iż z czasem ich rozmiar maleje. Widzimy także nieco mocniejszą ujemną zależnośc czasu oraz liczby złowionych śledzi.  
- możemy również dostrzec dość silną ujemną zależność rozmiaru śledzi oraz *sst* oznaczające temperaturę przy powierzchni wody. Oznacza to że wzraz ze wzrostem temperatury rozmiar śledzi się zmniejsza. 
- natomiast zwiększenie temperatury na powierzchni wody wykazuje pozytywną korelację z *nao* - oscylacją północno atlantycką. W związku z tym, być może to ona właśnie przyczynia się do jej ocieplenia.

Poniżej dla zobrazowania zależności rozmiaru śledzi od innych czynników przedstawione zostały dwa przykładowe wykresy:

```{r}
p <- ggplot(data = numeric_data[sample(nrow(numeric_data), 1000), c(2, 13)], aes(x = sst, y = length)) +
  geom_point() +
  geom_smooth()
p
```

- wykres zależności długości śledzia od temperatury przy powierzchni wody, potwierdza obserwację z macierzy korelacji - widać trend, że wzraz ze wzrostem temperatury rozmiar śledzi maleje.

```{r}
p <- ggplot(data = numeric_data[sample(nrow(numeric_data), 1000), c(2, 10)], aes(x = recr, y = length)) +
  geom_point() +
  geom_smooth()
p
```

- wykres zależności długości śledzia od liczby śledzi, również potwierdza obserwację z macierzy korelacji - nie widać żadnej konkretnej zależności.

## Zmiana rozmiaru śledzi w czasie <a name="rozmiar"></a>
```{r}
length_in_time <- raw_data[sample(nrow(raw_data), 1000), 1:2]
length_in_time <- data.frame(sapply(length_in_time, as.numeric))

p <- ggplot(data = length_in_time, aes(x = t, y = length)) +
  geom_point() +
  geom_smooth()

p <- ggplotly(p)

p
```

Powyższy interaktywny wykres przedstawia zależność rozmiaru śledzi od czasu. Z względów zasobowych pobrana została próbka 1000 przykładów spośród wszystkich 52582 (aby uzyskać lepsze uogólnienie, było to możliwe, ponieważ każdy rekord zawierał pomiar długości śledzia). Aby wyświetlić dokładną wartość interesującego użytkownika punktu, może on najechać myszką na dany obszar i odczytać wartość.

*Smoothed conditional means*, czyli wygładzona średnia długości śledzia w czasie została dodana do wykresu, aby łatwiej było zauważyć wzorce występujące w danych. Do wygładzania została użyta domyślna metoda *gam* oraz formuła *y ~ s(x, bs = "cs")*. Poświata która znajduje się pod wygładzoną średnią oznacza niski oraz wysoki punktowy przedział ufności wokół średniej.

Dzięki linii trendu na wykresie można z łatwością zaobserować, że do około 15000 przykładu rozmiar śledzia rósł, a później zaczął stopniowo spadać, aż osiągnął średnie minimum w ostatnim przykładzie.

## Regresor przewidujący rozmiar śledzia <a name="regresor"></a>
### Przygotowanie zbioru trenującego i testowego <a name="podzial"></a>

Aby poprawnie dobrać parametry modelu oraz oszacować jego skuteczność, zbiór danych został podzielony na dane uczące, walidujące i testowe w stosunku 0.56 do 0.24 do 0.20. W rezultacie otrzymane zostały następujące rozmiary zbiorów:

```{r, echo=FALSE}
TRAIN_PERCENT <- 0.8
VALIDATION_PERCENT <- 0.3

train_size <- floor(TRAIN_PERCENT * rows_number)
train_idx <- sample(seq_len(rows_number), size = train_size)

# drop "t"
train <- numeric_data[train_idx,-1]
test <- numeric_data[-train_idx,-1]

validation_size <- floor(VALIDATION_PERCENT * nrow(train))
validation_idx <- sample(seq_len(nrow(train)), size = validation_size)
validation <- train[validation_idx,]
train <- train[-validation_idx,]

print("Rozmiary zbiorów:")
printf("Trenujący: %d | Walidacyjny: %d | Testowy: %d", nrow(train), nrow(validation), nrow(test))
```

### Regresja liniowa <a name="linear-regression"></a>
```{r, echo=FALSE}
calculate_rsq <- function(pred, actual) {
  rss <- sum((pred - actual) ^ 2)
  tss <- sum((actual - mean(actual)) ^ 2)
  return(1 - rss/tss)
}

calculate_rmse <- function(pred, actual, N) {
  return(sqrt(sum((pred - actual) ^ 2) / N))
}

lm_mod <- lm(length ~ ., data = train)
pred <- predict(lm_mod, validation)
summary(lm_mod)
# residual standard error: 1.362 on 52567 degrees of freedom
```

```{r}
printf("RMSE: %f, R2: %f",  calculate_rmse(pred, validation$length, length(pred)), calculate_rsq(pred, validation$length))
```

### Regresja nieliniowa (wielomianowa) <a name="nonlinear-regression"></a>
```{r}
lm_mod <- lm(length ~ poly(cfin1, 2) + poly(cfin2, 2) + poly(chel1, 2) + poly(chel2, 2)  + poly(lcop1, 2)  + poly(lcop2, 2) + poly(fbar, 2) + poly(recr, 2) + poly(cumf, 2) + poly(totaln, 2) + poly(sst, 2) + poly(sal, 2) + poly(xmonth, 2) + poly(nao, 2) , data = train)

#lm_mod <- lm(length ~ poly(cfin1, 2) + poly(cfin2, 2) + poly(chel1, 2) + poly(chel2, 2)  + poly(lcop1, 2) + poly(fbar, 2) + poly(recr, 2) + poly(cumf, 2) + poly(totaln, 2) + poly(sst, 2) + poly(sal, 2) + poly(xmonth, 2) + poly(nao, 2) , data = train)

pred <- predict(lm_mod, validation)
printf("RMSE: %f, R2: %f",  calculate_rmse(pred, validation$length, length(pred)), calculate_rsq(pred, validation$length))
summary(lm_mod)
```

```{r}
lm_mod <- lm(length ~ poly(cfin1, 3) + poly(cfin2, 3) + poly(chel1, 3) + poly(chel2, 3)  + poly(lcop1, 3)  + poly(lcop2, 3) + poly(fbar, 3) + poly(recr, 3) + poly(cumf, 3) + poly(totaln, 3) + poly(sst, 3) + poly(sal, 3) + poly(xmonth, 3) + poly(nao, 3) , data = train)

#lm_mod <- lm(length ~ poly(cfin1, 2) + poly(cfin2, 2) + poly(chel1, 2) + poly(chel2, 2)  + poly(lcop1, 2) + poly(fbar, 2) + poly(recr, 2) + poly(cumf, 2) + poly(totaln, 2) + poly(sst, 2) + poly(sal, 2) + poly(xmonth, 2) + poly(nao, 2) , data = train)

pred <- predict(lm_mod, validation)
printf("RMSE: %f, R2: %f",  calculate_rmse(pred, validation$length, length(pred)), calculate_rsq(pred, validation$length))
summary(lm_mod)
```

### Regresja Ridge, Lasso i Elastic-Net <a name="ela-regression"></a>
```{r}
x_train_vars <- model.matrix(length~. , train)[,-1]
x_val_vars <- model.matrix(length~. , validation)[,-1]

y_train_var <- train$length
y_val_var <- validation$length

list.of.fits <- list()
results <- data.frame()

for (i in 0:10) {
  fit.name <- paste0("alpha", i/10)
  
  list.of.fits[[fit.name]] <-
    cv.glmnet(x_train_vars, y_train_var, type.measure="mse", alpha=i/10, 
      family="gaussian")
  
  ## Use each model to predict 'y' given the Testing dataset
  predicted <- 
    predict(list.of.fits[[fit.name]], 
      s=list.of.fits[[fit.name]]$lambda.1se, newx=x_val_vars)
  
  ## Calculate the Mean Squared Error...
  rss <- sum((predicted - y_val_var) ^ 2)
  tss <- sum((y_val_var - mean(y_val_var)) ^ 2)
  rsq <- 1 - rss/tss
  rmse <- sqrt(mean((y_val_var - predicted)^2))
  
  ## Store the results
  temp <- data.frame(alpha=i/10, rmse=rmse, rsq=rsq, fit.name=fit.name)
  results <- rbind(results, temp)
}
print(results)
```

## Gradient Boosting Machine <a name="yoyo"></a>
Kolejnym algorytmem który można wykorzystać do zadania regresji jest Gradient Boosting Machine. Jest to algorytm z rodziny tak zwanych *ensemble methods*, metod wykorzystujących wiele algorytmów uczenia celem uzyskania trafniejszych wyników. 

W rozważanym problemie skorzystałyśmy z kombinacji drzew decyzyjnych, co jest często spotykanym rozwiązaniem w kontekście Gradient Boosting Machine. Takie rozwiązanie jest zaimplementowane w bibliotece `gbm`.

Liczba drzew generowanych w modelu wynosi 2000. Każde drzewo opisuje parametr `interaction.depth`, który ma wartość 4 i określa że każde drzewo jest małym drzewem z 4 podziałami. Parametr `shrinkage` jest swego rodzaju prędkością uczenia (*learning rate*). 

```{r, cache=TRUE} 

NUM_TREES <- 2000
model <- gbm(length ~ . ,data = train,distribution = "gaussian",n.trees = NUM_TREES, shrinkage = 0.1, interaction.depth = 4)
model
```

```{r}
summary(model)
```

```{r}
n_trees_seq = seq(from=100, to=NUM_TREES, by=100)
pred <- predict(model, validation, n.trees=n_trees_seq)

mses <- with(validation, apply( (pred - length) ^ 2, 2, mean))
rmses <- sqrt(mses)

head(rmses)
```

```{r}
tail(rmses)
```


```{r}
#plot(n_trees_seq, mses, pch=19, col="blue", xlab="Number of Trees", ylab="Test Error", main="Efektywność boostingu na zbiorze walidacyjnym")
```

